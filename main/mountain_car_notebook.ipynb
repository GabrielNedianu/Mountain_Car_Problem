{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f494341-716d-421b-83db-8adea6ee6b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install ipywidgets matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833332be-df92-48a4-9d94-7a061219d978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 5000  # Total number of episodes\n",
    "epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c726e6-b1bb-45c0-bea3-af561baa11db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "class MountainCarAgent:\n",
    "    \"\"\"\n",
    "    Initialize the Mountain Car Agent.\n",
    "\n",
    "    Parameters:\n",
    "        alpha (float): The learning rate.\n",
    "        gamma (float): The discount factor for future rewards.\n",
    "        epsilon (float): The initial probability of taking a random action (exploration).\n",
    "        epsilon_decay (float): The decay rate of epsilon after each episode.\n",
    "        min_epsilon (float): The minimum value to which epsilon can decay.\n",
    "        num_episodes (int): The total number of episodes for training.\n",
    "        num_states (list): The number of discrete states in each dimension.\n",
    "\n",
    "    The constructor sets up the environment, initializes the Q-table, and configures the agent's learning and exploration parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=0.15, epsilon_decay=0.9, min_epsilon=0.01, num_episodes=1001, num_states=[20, 20]):\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_states = num_states\n",
    "        self.state_bounds = list(zip(self.env.observation_space.low, self.env.observation_space.high))\n",
    "        self.state_grid = [np.linspace(bound[0], bound[1], num_states[i]) for i, bound in enumerate(self.state_bounds)]\n",
    "        self.Q_table = np.zeros(self.num_states + [self.env.action_space.n])\n",
    "        self.total_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"\n",
    "        Discretize a continuous state into its corresponding grid indices.\n",
    "\n",
    "        Parameters:\n",
    "            state (tuple): The continuous state to be discretized.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A discretized state represented by indices corresponding to each dimension.\n",
    "\n",
    "        This method maps continuous state values to a discrete grid, enabling the use of a tabular Q-learning approach.\n",
    "        \"\"\"        \n",
    "        discretized_state = []\n",
    "        for s, grid in zip(state, self.state_grid):\n",
    "            index = np.digitize(s, grid) - 1\n",
    "            index = max(0, min(index, len(grid) - 1))\n",
    "            discretized_state.append(index)\n",
    "        return tuple(discretized_state)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Determine an action using an epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "            state (tuple): The current state from which the action needs to be decided.\n",
    "\n",
    "        Returns:\n",
    "            int: The action chosen based on the epsilon-greedy policy.\n",
    "\n",
    "        With a probability of epsilon, a random action is chosen (exploration). Otherwise, the best known action (exploitation) is chosen based on the Q-table.\n",
    "        \"\"\"        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])  # Exploit\n",
    "\n",
    "    def create_directory(self, dir_path):\n",
    "        \"\"\"\n",
    "        Create a directory if it doesn't exist.\n",
    "\n",
    "        Parameters:\n",
    "            dir_path (str): The path of the directory to be created.\n",
    "\n",
    "        This method checks if a directory exists at the given path, and if not, it creates it.\n",
    "        \"\"\"                \n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "    def save_Q_table(self, filename, folder=\"mountain_car/q_tables\"):\n",
    "        \"\"\"\n",
    "        Save the current Q-table to a file.\n",
    "\n",
    "        Parameters:\n",
    "            filename (str): The name of the file to save the Q-table.\n",
    "            folder (str): The folder where the Q-table should be saved.\n",
    "\n",
    "        This method saves the Q-table into a specified file for later use, such as resuming training or analysis.\n",
    "        \"\"\"        \n",
    "        self.create_directory(folder)\n",
    "        np.save(os.path.join(folder, filename), self.Q_table)\n",
    "\n",
    "    def load_Q_table(self, filename, folder=\"mountain_car/q_tables\"):\n",
    "        \"\"\"\n",
    "        Load a Q-table from a file.\n",
    "\n",
    "        Parameters:\n",
    "            filename (str): The name of the file containing the Q-table.\n",
    "            folder (str): The folder where the Q-table is located.\n",
    "\n",
    "        This method loads a Q-table from a file, allowing the agent to use a pre-trained policy.\n",
    "        \"\"\"            \n",
    "        self.Q_table = np.load(filename)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using the Q-learning algorithm.\n",
    "\n",
    "        During training, the agent interacts with the environment, updates the Q-table based on the received rewards, and gradually improves its policy. Epsilon is decayed after each episode to reduce exploration over time.\n",
    "        \"\"\"        \n",
    "        for episode in range(self.num_episodes):\n",
    "            state_raw, _ = self.env.reset()\n",
    "            state = self.discretize_state(state_raw)\n",
    "            total_reward, steps = 0, 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.epsilon_greedy_policy(state)\n",
    "                next_state_raw, reward, done, _, _ = self.env.step(action)\n",
    "                next_state = self.discretize_state(next_state_raw)\n",
    "\n",
    "                # Q-table update\n",
    "                best_next_action = np.argmax(self.Q_table[next_state])\n",
    "                td_target = reward + self.gamma * self.Q_table[next_state][best_next_action] # Temporal Difference Target\n",
    "                self.Q_table[state][action] += self.alpha * (td_target - self.Q_table[state][action])\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "            self.total_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(steps)\n",
    "            \n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)\n",
    "\n",
    "            if self.num_episodes < 1500 and episode % 100 == 0:\n",
    "                print(f\"Episode: {episode}, Average Reward: {np.mean(self.total_rewards[-100:])}\")\n",
    "            elif self.num_episodes < 10500 and episode % 500 == 0:\n",
    "                print(f\"Episode: {episode}, Average Reward: {np.mean(self.total_rewards[-500:])}\")\n",
    "            elif self.num_episodes > 10500 and episode % 2500 == 0:\n",
    "                print(f\"Episode: {episode}, Average Reward: {np.mean(self.total_rewards[-2500:])}\")\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def plot_and_save_Q_table_analysis(self, test_id = 0, folder=\"mountain_car/tests/learning\"):\n",
    "        \"\"\"\n",
    "        Generate and save plots analyzing the Q-table.\n",
    "\n",
    "        Parameters:\n",
    "            test_id (int): An identifier for the test scenario.\n",
    "            folder (str): The folder where the plots should be saved.\n",
    "\n",
    "        This method creates a series of plots to analyze the learning process of the agent, including the learning curve, episode lengths, and Q-value heatmaps, and saves them to files, in the given folder or default to mountain_car/tests/learning.\n",
    "        \"\"\"        \n",
    "        self.create_directory(folder)\n",
    "        num_positions = len(self.state_grid[0])\n",
    "        num_velocities = len(self.state_grid[1])\n",
    "        policy = np.argmax(self.Q_table, axis=2)\n",
    "\n",
    "        positions = np.linspace(-1.2, 0.6, num_positions)\n",
    "        velocities = np.linspace(-0.07, 0.07, num_velocities)\n",
    "        pos_grid, vel_grid = np.meshgrid(positions, velocities)\n",
    "        action_colors = {0: 'red', 1: 'green', 2: 'blue'}\n",
    "        color_array = np.array([[action_colors[action] for action in row] for row in policy])\n",
    "\n",
    "        plt.figure(figsize=(18, 24))\n",
    "        \n",
    "        # Subplot 1: Learning Curve\n",
    "        plt.subplot(5, 2, 1)\n",
    "        plt.plot(self.total_rewards)  \n",
    "        plt.title('Learning Curve: Total Reward per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "\n",
    "        # Subplot 2: Episode Length Over Time\n",
    "        plt.subplot(5, 2, 2)\n",
    "        plt.plot(self.episode_lengths)  \n",
    "        plt.title('Episode Length Over Time')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Length of Episode')\n",
    "\n",
    "        # Subplot 3: Heatmap for First Action\n",
    "        selected_action = 0  \n",
    "        plt.subplot(5, 2, 3)\n",
    "        sns.heatmap(self.Q_table[:, :, selected_action])  # Replace with your Q_table data\n",
    "        plt.title(f'Heatmap of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Subplot 4: 3D Surface Plot for the Same Action\n",
    "        ax = plt.subplot(5, 2, 4, projection='3d')\n",
    "        X, Y = np.meshgrid(np.arange(self.Q_table.shape[0]), np.arange(self.Q_table.shape[1]))\n",
    "        Z = self.Q_table[:, :, selected_action]  # Replace with your Q_table data\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "        ax.set_title(f'3D Surface Plot of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        ax.set_zlabel('Q-Value')\n",
    "\n",
    "        # Subplot 5: Heatmap for Second Action\n",
    "        selected_action = 1\n",
    "        plt.subplot(5, 2, 5)\n",
    "        sns.heatmap(self.Q_table[:, :, selected_action])  # Replace with your Q_table data\n",
    "        plt.title(f'Heatmap of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Subplot 6: 3D Surface Plot for the Same Action\n",
    "        ax = plt.subplot(5, 2, 6, projection='3d')\n",
    "        X, Y = np.meshgrid(np.arange(self.Q_table.shape[0]), np.arange(self.Q_table.shape[1]))\n",
    "        Z = self.Q_table[:, :, selected_action]  \n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "        ax.set_title(f'3D Surface Plot of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        ax.set_zlabel('Q-Value')\n",
    "\n",
    "        # Subplot 7: Heatmap for Third Action\n",
    "        selected_action = 2\n",
    "        plt.subplot(5, 2, 7)\n",
    "        sns.heatmap(self.Q_table[:, :, selected_action]) \n",
    "        plt.title(f'Heatmap of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Subplot 8: 3D Surface Plot for the Same Action\n",
    "        ax = plt.subplot(5, 2, 8, projection='3d')\n",
    "        X, Y = np.meshgrid(np.arange(self.Q_table.shape[0]), np.arange(self.Q_table.shape[1]))\n",
    "        Z = self.Q_table[:, :, selected_action]  \n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "        ax.set_title(f'3D Surface Plot of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        ax.set_zlabel('Q-Value')\n",
    "\n",
    "        # Subplot 9: Policy Map (Color-Coded Actions)\n",
    "        plt.subplot(5, 2, 9)\n",
    "        cmap_colors = [action_colors[i] for i in sorted(action_colors)]\n",
    "        cmap = sns.color_palette(cmap_colors, as_cmap=True)\n",
    "        policy_heatmap = sns.heatmap(policy, cmap=cmap, annot=False)  # 'policy' and 'cmap' as defined earlier\n",
    "        plt.title(\"Policy Map (Color-Coded Actions)\")\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Subplot 10: Position and Velocity Policy Visualization\n",
    "        plt.subplot(5, 2, 10)\n",
    "        plt.scatter(pos_grid, vel_grid, color=color_array.ravel()) \n",
    "        plt.title('Policy Visualization for Mountain Car')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        patches = [mpatches.Patch(color=color, label=label) for label, color in action_colors.items()]  \n",
    "        plt.legend(handles=patches, title=\"Actions\", title_fontsize='13', loc='best')\n",
    "\n",
    "        # # Adjust layout and show\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save the figure based on the test_id\n",
    "        plt.tight_layout()\n",
    "        filename = f\"Q_table_analysis_test_{test_id}.png\"\n",
    "        plt.savefig(os.path.join(folder, filename))\n",
    "        plt.close()\n",
    "        \n",
    "    def test_policy(self, num_episodes=1000, max_steps=1000):\n",
    "        \"\"\"\n",
    "        Test the trained policy over a number of episodes.\n",
    "\n",
    "        Parameters:\n",
    "            num_episodes (int): The number of episodes to test the policy.\n",
    "            max_steps (int): The maximum number of steps per episode.\n",
    "\n",
    "        Returns:\n",
    "            float: The success rate of the agent.\n",
    "\n",
    "        This method tests the agent's policy by running it through several episodes and calculates the rate of successful episodes.\n",
    "        \"\"\"        \n",
    "        success_count = 0\n",
    "        for episode in range(num_episodes):\n",
    "            state_raw, _ = self.env.reset()\n",
    "            state = self.discretize_state(state_raw)\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < max_steps:\n",
    "                action = np.argmax(self.Q_table[state])\n",
    "                state_raw, _, done, _, _ = self.env.step(action)\n",
    "                state = self.discretize_state(state_raw)\n",
    "                step += 1\n",
    "                if done and state_raw[0] >= 0.5:  # Check if goal is reached\n",
    "                    success_count += 1\n",
    "\n",
    "        success_rate = success_count / num_episodes\n",
    "        return success_rate\n",
    "    \n",
    "    def test_policy_analyzed(self, num_episodes=10000, max_steps=1000):\n",
    "        \"\"\"\n",
    "        Test the policy and gather detailed analysis data.\n",
    "\n",
    "        Parameters:\n",
    "            num_episodes (int): The number of episodes for testing.\n",
    "            max_steps (int): The maximum number of steps per episode.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing detailed analysis data such as success rates, episode lengths, rewards, state visits, and outcomes.\n",
    "\n",
    "        This method performs an extensive test of the agent's policy, collecting data for further in-depth analysis.\n",
    "        \"\"\"        \n",
    "        success_count = 0\n",
    "        episode_lengths = []\n",
    "        episode_rewards = []\n",
    "        grid_x, grid_y = self.num_states\n",
    "        state_visits = np.zeros(grid_x * grid_y)\n",
    "        episode_end_states = []\n",
    "        episode_outcomes = []\n",
    "        failures = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state_raw, _ = self.env.reset()\n",
    "            state = self.discretize_state(state_raw)\n",
    "            done = False\n",
    "            success = False\n",
    "            step = 0\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done and step < max_steps:\n",
    "                action = np.argmax(self.Q_table[state])\n",
    "                state_raw, reward, done, _, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(state_raw)\n",
    "                total_reward += reward\n",
    "                x, y = new_state\n",
    "                single_index = y * grid_x + x\n",
    "                state_visits[single_index] += 1\n",
    "\n",
    "                if done and state_raw[0] < 0.5:\n",
    "                    failures.append((episode, state_raw, step))\n",
    "\n",
    "                state = new_state\n",
    "                step += 1\n",
    "\n",
    "                if done and state_raw[0] >= 0.5:\n",
    "                    success = True\n",
    "                    success_count += 1\n",
    "\n",
    "            episode_lengths.append(step)\n",
    "            episode_rewards.append(total_reward)\n",
    "            episode_end_states.append(state_raw)\n",
    "            episode_outcomes.append(success)\n",
    "\n",
    "        success_rate = success_count / num_episodes\n",
    "        analysis_data = {\n",
    "            'success_rate': success_rate,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'state_visits': state_visits,\n",
    "            'episode_end_states': episode_end_states,\n",
    "            'failures': failures,\n",
    "            'episode_outcomes': episode_outcomes,\n",
    "            'episodes' : num_episodes\n",
    "        }\n",
    "        return analysis_data\n",
    "    \n",
    "    def plot_analysis_data(self, analysis_data, test_id = 0, folder=\"mountain_car/tests/analysis\"):\n",
    "        \"\"\"\n",
    "        Generate and save plots based on analysis data.\n",
    "\n",
    "        Parameters:\n",
    "            analysis_data (dict): The analysis data obtained from testing the policy.\n",
    "            test_id (int): An identifier for the test scenario.\n",
    "            folder (str): The folder where the plots should be saved.\n",
    "\n",
    "        This method visualizes the performance and behavior of the agent using the provided analysis data and saves the resulting plots.\n",
    "        \"\"\"        \n",
    "        self.create_directory(folder)\n",
    "        plt.figure(figsize=(15, 20))\n",
    "\n",
    "        # Scatter Plot of End States Color-Coded by Success or Failure\n",
    "        end_states = np.array(analysis_data['episode_end_states'])\n",
    "        outcomes = np.array(analysis_data['episode_outcomes'])\n",
    "        cmap = mcolors.ListedColormap(['red', 'green'])\n",
    "        binary_outcomes = np.where(outcomes, 1, 0)\n",
    "        plt.subplot(3, 2, 1)\n",
    "        plt.scatter(end_states[:, 0], end_states[:, 1], c=binary_outcomes, cmap=cmap)\n",
    "        plt.title('End States Scatter Plot (Red=Failure, Green=Succes)')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Histogram of Episode Lengths\n",
    "        plt.subplot(3, 2, 2)\n",
    "        plt.hist(analysis_data['episode_lengths'], bins=30, color='skyblue')\n",
    "        plt.title('Histogram of Episode Lengths')\n",
    "        plt.xlabel('Episode Length')\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "        # Line Plot of Episode Rewards\n",
    "        plt.subplot(3, 2, 3)\n",
    "        plt.plot(analysis_data['episode_rewards'], color='green')\n",
    "        plt.title('Episode Rewards Over Time')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "\n",
    "        # Heatmap of State Visits\n",
    "        plt.subplot(3, 2, 4)\n",
    "        state_visits_reshaped = np.reshape(analysis_data['state_visits'], self.num_states)\n",
    "        sns.heatmap(state_visits_reshaped, cmap='viridis', annot=False)\n",
    "        plt.title('Heatmap of State Visits')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Simple plot to display success or failure for each episode\n",
    "        plt.subplot(3, 2, 5)\n",
    "        plt.plot(binary_outcomes, marker='o', linestyle='', color='blue')\n",
    "        plt.title('Success (1) or Failure (0) Per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Outcome (1=Success, 0=Failure)')\n",
    "        plt.yticks([0, 1], ['Failure', 'Success'])\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Line plot for episode lengths\n",
    "        plt.subplot(3, 2, 6)\n",
    "        plt.plot(analysis_data['episode_lengths'], color='purple')\n",
    "        plt.title('Episode Lengths Over Time')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Length of Episode')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # # Adjust layout and show\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f\"analysis_plots_test_{test_id}_{analysis_data['episodes']}_episodes.png\"\n",
    "        plt.savefig(os.path.join(folder, filename))\n",
    "        plt.close()\n",
    "        \n",
    "    def test_policy_with_visualization(self, num_episodes=5):\n",
    "        \"\"\"\n",
    "        Visualize the agent's policy for a specified number of episodes.\n",
    "\n",
    "        Parameters:\n",
    "            num_episodes (int): The number of episodes to visualize.\n",
    "\n",
    "        This method runs the agent for a few episodes and visually shows the agent's decisions in the Mountain Car environment.\n",
    "        \"\"\"        \n",
    "        for episode in range(num_episodes):\n",
    "            state_raw, _ = self.env.reset()\n",
    "            state = self.discretize_state(state_raw)\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                action = np.argmax(self.Q_table[state])\n",
    "                action_taken = \"Turn Left\" if action == 0 else (\"Do Nothing\" if action == 1 else \"Turn Right\")\n",
    "                self.custom_render(step, action_taken)\n",
    "                state_raw, _, done, _, _ = self.env.step(action)\n",
    "                state = self.discretize_state(state_raw)\n",
    "                step += 1\n",
    "           \n",
    "    def custom_render(self, step, action_taken):\n",
    "        \"\"\"\n",
    "        Custom function to visualize the state of the environment.\n",
    "\n",
    "        Parameters:\n",
    "            step (int): The current step number in the episode.\n",
    "            action_taken (str): The action taken by the agent at this step.\n",
    "\n",
    "        This method creates a visual representation of the Mountain Car environment, including the current position of the car and the action taken.\n",
    "        \"\"\"        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        positions = np.linspace(self.env.unwrapped.min_position, self.env.unwrapped.max_position, 100)\n",
    "        mountain_heights = np.sin(3 * positions)\n",
    "        plt.plot(positions, mountain_heights, color='gray', linewidth=2)\n",
    "        car_position = self.env.unwrapped.state[0]\n",
    "        plt.scatter(car_position, np.sin(3 * car_position), c='blue', marker='o', s=200)\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.ylabel(\"Mountain Height\")\n",
    "        plt.title(f\"MountainCarEnv - Step {step} - Action {action_taken}\")\n",
    "        plt.xlim(self.env.unwrapped.min_position, self.env.unwrapped.max_position)\n",
    "        plt.ylim(-1.2, 1.2)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage with default values\n",
    "default_agent = MountainCarAgent()\n",
    "default_agent.train()\n",
    "\n",
    "# Run a simple test with 100 episodes\n",
    "success_rate = default_agent.test_policy(num_episodes=100)\n",
    "print(f\"Success rate with 1000 episodes: {success_rate * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8530e5-7e8b-401a-8118-34e35d3c0426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.9, 'min_epsilon': 0.01, 'num_episodes': 5001, 'num_states': [20, 20]}\n",
      "Episode: 0, Average Reward: -11371.0\n",
      "Episode: 500, Average Reward: -503.568\n",
      "Episode: 1000, Average Reward: -285.328\n",
      "Episode: 1500, Average Reward: -202.41\n",
      "Episode: 2000, Average Reward: -204.802\n",
      "Episode: 2500, Average Reward: -189.836\n",
      "Episode: 3000, Average Reward: -182.776\n",
      "Episode: 3500, Average Reward: -182.64\n",
      "Episode: 4000, Average Reward: -202.424\n",
      "Episode: 4500, Average Reward: -183.982\n",
      "Episode: 5000, Average Reward: -183.592\n",
      "Training complete!\n",
      "Success rate of the learned policy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "# Usage with custom values\n",
    "\n",
    "config = {\n",
    "    'alpha': 0.1,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon': 0.3,\n",
    "    'epsilon_decay': 0.9,\n",
    "    'min_epsilon': 0.01,\n",
    "    'num_episodes': 5001,\n",
    "    'num_states': [20,20]\n",
    "}\n",
    "\n",
    "print(f\"Testing configuration: {config}\")\n",
    "\n",
    "custom_agent = MountainCarAgent(**config)\n",
    "custom_agent.train()\n",
    "\n",
    "# Uncomment this to save the q-table analysys\n",
    "#custom_agent.plot_and_save_Q_table_analysis(100)\n",
    "\n",
    "success_rate = custom_agent.test_policy(num_episodes = 100)\n",
    "print(f\"Success rate of the learned policy: {success_rate * 100:.2f}%\")\n",
    "\n",
    "# Uncomment this to run and save the graphics for an analyzed test\n",
    "#analysis_data = custom_agent.test_policy_analyzed(num_episodes = 100)\n",
    "#custom_agent.plot_analysis_data(analysis_data)\n",
    "# print(f\"Success Rate: {analysis_data['success_rate'] * 100:.2f}%, \"\n",
    "#               f\"Average Episode Length: {np.mean(analysis_data['episode_lengths']):.2f} steps, \"\n",
    "#               f\"Min Episode Length: {np.min(analysis_data['episode_lengths'])} steps, \"\n",
    "#               f\"Average Reward: {np.mean(analysis_data['episode_rewards']):.2f}, \"\n",
    "#               f\"Failures: {len(analysis_data['failures'])}, \"\n",
    "#               f\"Total Episodes: {analysis_data['episodes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c882777-b801-486d-8e2f-760f959a6adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run with visualization \n",
    "custom_agent.test_policy_with_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479396b9-1a4f-4202-a815-7ab93ac7ef6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tests for comparation between different approaches\n",
    "\n",
    "configs = [\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [10, 10]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [10, 10]},\n",
    "    \n",
    "    {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [10, 10]},  \n",
    "    {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [10, 10]},  \n",
    "    \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [20, 20]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [20, 20]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [20, 20]},\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 25000, 'num_states': [20, 20]},\n",
    "    \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [20, 20]},  \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [20, 20]},  \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [20, 20]},  \n",
    "    \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [50, 50]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [50, 50]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]},\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [50, 50]},\n",
    "    \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [50, 50]},  \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [50, 50]},  \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]},\n",
    "    {'alpha': 0.5, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.9, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]},\n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [50, 50]},\n",
    "    \n",
    "    {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [100, 100]},  \n",
    "    {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 25000, 'num_states': [100, 100]},\n",
    "    \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.4, 'epsilon_decay': 0.99, 'min_epsilon': 0.01, 'num_episodes': 100000, 'num_states': [50, 50]},\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.99, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [20, 20]},\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [10, 10]},\n",
    "    {'alpha': 0.25, 'gamma': 0.5, 'epsilon': 0.4, 'epsilon_decay': 0.9, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [50, 50]},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Test for each configuration\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"Testing configuration {i+1}: {config}\")\n",
    "\n",
    "    # Initialize and train the agent\n",
    "    agent = MountainCarAgent(**config)\n",
    "    agent.train()\n",
    "\n",
    "    # Save Q-table plot\n",
    "    agent.plot_and_save_Q_table_analysis(test_id=i+1)\n",
    "\n",
    "    # Run a simple test with 1000 episodes\n",
    "    success_rate = agent.test_policy(num_episodes=1000)\n",
    "    print(f\"Success rate with 1000 episodes: {success_rate * 100:.2f}%\")\n",
    "    \n",
    "    # Save the Q-table\n",
    "    q_table_filename = f\"q_table_config_{i+1}.npy\"\n",
    "    agent.save_Q_table(q_table_filename)\n",
    "    print(f\"Q-table for configuration {i+1} saved.\")\n",
    "\n",
    "    # Run analysis tests with different number of episodes\n",
    "    for num_episodes in [100, 500, 1000, 10000]:\n",
    "        analysis_data = agent.test_policy_analyzed(num_episodes=num_episodes)\n",
    "        agent.plot_analysis_data(analysis_data, test_id=i+1)\n",
    "        print(f\"Analysis for {num_episodes} episodes saved.\")\n",
    "        print(f\"Success Rate: {analysis_data['success_rate'] * 100:.2f}%, \"\n",
    "              f\"Average Episode Length: {np.mean(analysis_data['episode_lengths']):.2f} steps, \"\n",
    "              f\"Min Episode Length: {np.min(analysis_data['episode_lengths'])} steps, \"\n",
    "              f\"Average Reward: {np.mean(analysis_data['episode_rewards']):.2f}, \"\n",
    "              f\"Failures: {len(analysis_data['failures'])}, \"\n",
    "              f\"Total Episodes: {analysis_data['episodes']}\")\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20872899-64fa-49a8-84d7-0df119ca54a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reevaluate a case for later vizualization\n",
    "\n",
    "i = 24\n",
    "config = {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.4, 'epsilon_decay': 0.99, 'min_epsilon': 0.01, 'num_episodes': 150000, 'num_states': [100, 100]}\n",
    "\n",
    "# Initialize and train the agent\n",
    "agent = MountainCarAgent(**config)\n",
    "agent.train()\n",
    "\n",
    "# Save Q-table plot\n",
    "agent.plot_and_save_Q_table_analysis(test_id=i)\n",
    "\n",
    "# Run a simple test with 1000 episodes\n",
    "success_rate = agent.test_policy(num_episodes=1000)\n",
    "print(f\"Success rate with 1000 episodes: {success_rate * 100:.2f}%\")\n",
    "\n",
    "# Save the Q-table\n",
    "q_table_filename = f\"q_table_config_{i}.npy\"\n",
    "agent.save_Q_table(q_table_filename)\n",
    "print(f\"Q-table for configuration {i} saved.\")\n",
    "\n",
    "# Run analysis tests with different number of episodes\n",
    "for num_episodes in [100, 500, 1000, 10000]:\n",
    "    analysis_data = agent.test_policy_analyzed(num_episodes=num_episodes)\n",
    "    agent.plot_analysis_data(analysis_data, test_id=i)\n",
    "    print(f\"Analysis for {num_episodes} episodes saved.\")\n",
    "    print(f\"Success Rate: {analysis_data['success_rate'] * 100:.2f}%, \"\n",
    "          f\"Average Episode Length: {np.mean(analysis_data['episode_lengths']):.2f} steps, \"\n",
    "          f\"Min Episode Length: {np.min(analysis_data['episode_lengths'])} steps, \"\n",
    "          f\"Average Reward: {np.mean(analysis_data['episode_rewards']):.2f}, \"\n",
    "          f\"Failures: {len(analysis_data['failures'])}, \"\n",
    "          f\"Total Episodes: {analysis_data['episodes']}\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b055729-f058-4a4f-9452-f6eeeb863082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.test_policy_with_visualization(num_episodes=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
