{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f494341-716d-421b-83db-8adea6ee6b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install ipywidgets matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833332be-df92-48a4-9d94-7a061219d978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 5000  # Total number of episodes\n",
    "epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "total_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "21c726e6-b1bb-45c0-bea3-af561baa11db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "class MountainCarAgent:\n",
    "    \"\"\"Initialize the agen, Q_table, create the state_grid\"\"\"\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=0.15, epsilon_decay=0.9, min_epsilon=0.01, num_episodes=1001, num_states=[20, 20]):\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_states = num_states\n",
    "        self.state_bounds = list(zip(self.env.observation_space.low, self.env.observation_space.high))\n",
    "        self.state_grid = [np.linspace(bound[0], bound[1], num_states[i]) for i, bound in enumerate(self.state_bounds)]\n",
    "        self.Q_table = np.zeros(self.num_states + [self.env.action_space.n])\n",
    "        self.total_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    \"\"\"Convert a continuous state to its discretized form.\"\"\"\n",
    "    def discretize_state(self, state):\n",
    "        discretized_state = []\n",
    "        for s, grid in zip(state, self.state_grid):\n",
    "            index = np.digitize(s, grid) - 1\n",
    "            index = max(0, min(index, len(grid) - 1))\n",
    "            discretized_state.append(index)\n",
    "        return tuple(discretized_state)\n",
    "\n",
    "    \"\"\"Return an action based on the epsilon-greedy policy.\"\"\"\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if np.random.random() < epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])  # Exploit\n",
    "\n",
    "    \"\"\"Create a directory if it doesn't exist.\"\"\"\n",
    "    def create_directory(self, dir_path):\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "    \"\"\"Save the Q_table for later use.\"\"\"\n",
    "    def save_Q_table(self, filename, folder=\"mountain_car/q_tables\"):\n",
    "        self.create_directory(folder)\n",
    "        np.save(os.path.join(folder, filename), self.Q_table)\n",
    "\n",
    "    \"\"\"Load the Q_table to be used.\"\"\"\n",
    "    def load_Q_table(self, filename, folder=\"mountain_car/q_tables\"):\n",
    "            self.Q_table = np.load(filename)\n",
    "\n",
    "    \"\"\"Train the agent using Q-learning.\"\"\"\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            state_raw, _ = self.env.reset()\n",
    "            state = self.discretize_state(state_raw)\n",
    "            total_reward, steps = 0, 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.epsilon_greedy_policy(state)\n",
    "                next_state_raw, reward, done, _, _ = self.env.step(action)\n",
    "                next_state = self.discretize_state(next_state_raw)\n",
    "\n",
    "                # Q-table update\n",
    "                best_next_action = np.argmax(self.Q_table[next_state])\n",
    "                td_target = reward + self.gamma * self.Q_table[next_state][best_next_action]\n",
    "                self.Q_table[state][action] += self.alpha * (td_target - self.Q_table[state][action])\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "            self.total_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(steps)\n",
    "            \n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)\n",
    "\n",
    "            if self.num_episodes < 1500 and episode % 100 == 0:\n",
    "                print(f\"Episode: {episode}, Average Reward: {np.mean(self.total_rewards[-100:])}\")\n",
    "            elif self.num_episodes < 10500 and episode % 500 == 0:\n",
    "                print(f\"Episode: {episode}, Average Reward: {np.mean(self.total_rewards[-500:])}\")\n",
    "            elif self.num_episodes > 10500 and episode % 2500 == 0:\n",
    "                print(f\"Episode: {episode}, Average Reward: {np.mean(self.total_rewards[-2500:])}\")\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    \"\"\" Plot the Q_table graphics and save them\"\"\"\n",
    "    def plot_and_save_Q_table_analysis(self, test_id = 0, folder=\"mountain_car/tests/learning\"):\n",
    "        self.create_directory(folder)\n",
    "        num_positions = len(self.state_grid[0])\n",
    "        num_velocities = len(self.state_grid[1])\n",
    "        policy = np.argmax(self.Q_table, axis=2)\n",
    "\n",
    "        positions = np.linspace(-1.2, 0.6, num_positions)\n",
    "        velocities = np.linspace(-0.07, 0.07, num_velocities)\n",
    "        pos_grid, vel_grid = np.meshgrid(positions, velocities)\n",
    "        action_colors = {0: 'red', 1: 'green', 2: 'blue'}\n",
    "        color_array = np.array([[action_colors[action] for action in row] for row in policy])\n",
    "\n",
    "        plt.figure(figsize=(18, 24))\n",
    "        \n",
    "        # Subplot 1: Learning Curve\n",
    "        plt.subplot(5, 2, 1)\n",
    "        plt.plot(self.total_rewards)  \n",
    "        plt.title('Learning Curve: Total Reward per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "\n",
    "        # Subplot 2: Episode Length Over Time\n",
    "        plt.subplot(5, 2, 2)\n",
    "        plt.plot(self.episode_lengths)  \n",
    "        plt.title('Episode Length Over Time')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Length of Episode')\n",
    "\n",
    "        # Subplot 3: Heatmap for First Action\n",
    "        selected_action = 0  \n",
    "        plt.subplot(5, 2, 3)\n",
    "        sns.heatmap(self.Q_table[:, :, selected_action])  # Replace with your Q_table data\n",
    "        plt.title(f'Heatmap of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Subplot 4: 3D Surface Plot for the Same Action\n",
    "        ax = plt.subplot(5, 2, 4, projection='3d')\n",
    "        X, Y = np.meshgrid(np.arange(self.Q_table.shape[0]), np.arange(self.Q_table.shape[1]))\n",
    "        Z = self.Q_table[:, :, selected_action]  # Replace with your Q_table data\n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "        ax.set_title(f'3D Surface Plot of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        ax.set_zlabel('Q-Value')\n",
    "\n",
    "        # Subplot 5: Heatmap for Second Action\n",
    "        selected_action = 1\n",
    "        plt.subplot(5, 2, 5)\n",
    "        sns.heatmap(self.Q_table[:, :, selected_action])  # Replace with your Q_table data\n",
    "        plt.title(f'Heatmap of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Subplot 6: 3D Surface Plot for the Same Action\n",
    "        ax = plt.subplot(5, 2, 6, projection='3d')\n",
    "        X, Y = np.meshgrid(np.arange(self.Q_table.shape[0]), np.arange(self.Q_table.shape[1]))\n",
    "        Z = self.Q_table[:, :, selected_action]  \n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "        ax.set_title(f'3D Surface Plot of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        ax.set_zlabel('Q-Value')\n",
    "\n",
    "        # Subplot 7: Heatmap for Third Action\n",
    "        selected_action = 2\n",
    "        plt.subplot(5, 2, 7)\n",
    "        sns.heatmap(self.Q_table[:, :, selected_action]) \n",
    "        plt.title(f'Heatmap of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Subplot 8: 3D Surface Plot for the Same Action\n",
    "        ax = plt.subplot(5, 2, 8, projection='3d')\n",
    "        X, Y = np.meshgrid(np.arange(self.Q_table.shape[0]), np.arange(self.Q_table.shape[1]))\n",
    "        Z = self.Q_table[:, :, selected_action]  \n",
    "        ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "        ax.set_title(f'3D Surface Plot of Q-Values for Action {selected_action}')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        ax.set_zlabel('Q-Value')\n",
    "\n",
    "        # Subplot 9: Policy Map (Color-Coded Actions)\n",
    "        plt.subplot(5, 2, 9)\n",
    "        cmap_colors = [action_colors[i] for i in sorted(action_colors)]\n",
    "        cmap = sns.color_palette(cmap_colors, as_cmap=True)\n",
    "        policy_heatmap = sns.heatmap(policy, cmap=cmap, annot=False)  # 'policy' and 'cmap' as defined earlier\n",
    "        plt.title(\"Policy Map (Color-Coded Actions)\")\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Subplot 10: Position and Velocity Policy Visualization\n",
    "        plt.subplot(5, 2, 10)\n",
    "        plt.scatter(pos_grid, vel_grid, color=color_array.ravel()) \n",
    "        plt.title('Policy Visualization for Mountain Car')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        patches = [mpatches.Patch(color=color, label=label) for label, color in action_colors.items()]  \n",
    "        plt.legend(handles=patches, title=\"Actions\", title_fontsize='13', loc='best')\n",
    "\n",
    "        # # Adjust layout and show\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save the figure based on the test_id\n",
    "        plt.tight_layout()\n",
    "        filename = f\"Q_table_analysis_test_{test_id}.png\"\n",
    "        plt.savefig(os.path.join(folder, filename))\n",
    "        plt.close()\n",
    "        \n",
    "    \"\"\"Test the trained policy over a number of episodes.\"\"\"        \n",
    "    def test_policy(self, num_episodes=1000, max_steps=1000):\n",
    "        success_count = 0\n",
    "        for episode in range(num_episodes):\n",
    "            state_raw, _ = self.env.reset()\n",
    "            state = self.discretize_state(state_raw)\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < max_steps:\n",
    "                action = np.argmax(self.Q_table[state])\n",
    "                state_raw, _, done, _, _ = self.env.step(action)\n",
    "                state = self.discretize_state(state_raw)\n",
    "                step += 1\n",
    "                if done and state_raw[0] >= 0.5:  # Check if goal is reached\n",
    "                    success_count += 1\n",
    "\n",
    "        success_rate = success_count / num_episodes\n",
    "        return success_rate\n",
    "    \n",
    "    \"\"\"Test the policy and gather detailed analysis data.\"\"\"\n",
    "    def test_policy_analyzed(self, num_episodes=10000, max_steps=1000):\n",
    "        success_count = 0\n",
    "        episode_lengths = []\n",
    "        episode_rewards = []\n",
    "        grid_x, grid_y = self.num_states\n",
    "        state_visits = np.zeros(grid_x * grid_y)\n",
    "        episode_end_states = []\n",
    "        episode_outcomes = []\n",
    "        failures = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state_raw, _ = self.env.reset()\n",
    "            state = self.discretize_state(state_raw)\n",
    "            done = False\n",
    "            success = False\n",
    "            step = 0\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done and step < max_steps:\n",
    "                action = np.argmax(self.Q_table[state])\n",
    "                state_raw, reward, done, _, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(state_raw)\n",
    "                total_reward += reward\n",
    "                x, y = new_state\n",
    "                single_index = y * grid_x + x\n",
    "                state_visits[single_index] += 1\n",
    "\n",
    "                if done and state_raw[0] < 0.5:\n",
    "                    failures.append((episode, state_raw, step))\n",
    "\n",
    "                state = new_state\n",
    "                step += 1\n",
    "\n",
    "                if done and state_raw[0] >= 0.5:\n",
    "                    success = True\n",
    "                    success_count += 1\n",
    "\n",
    "            episode_lengths.append(step)\n",
    "            episode_rewards.append(total_reward)\n",
    "            episode_end_states.append(state_raw)\n",
    "            episode_outcomes.append(success)\n",
    "\n",
    "        success_rate = success_count / num_episodes\n",
    "        analysis_data = {\n",
    "            'success_rate': success_rate,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'state_visits': state_visits,\n",
    "            'episode_end_states': episode_end_states,\n",
    "            'failures': failures,\n",
    "            'episode_outcomes': episode_outcomes,\n",
    "            'episodes' : num_episodes\n",
    "        }\n",
    "        return analysis_data\n",
    "    \n",
    "    \"\"\"Generate and save plots based on analysis data.\"\"\"\n",
    "    def plot_analysis_data(self, analysis_data, test_id = 0, folder=\"mountain_car/tests/analysis\"):\n",
    "        self.create_directory(folder)\n",
    "        plt.figure(figsize=(15, 20))\n",
    "\n",
    "        # Scatter Plot of End States Color-Coded by Success or Failure\n",
    "        end_states = np.array(analysis_data['episode_end_states'])\n",
    "        outcomes = np.array(analysis_data['episode_outcomes'])\n",
    "        cmap = mcolors.ListedColormap(['red', 'green'])\n",
    "        binary_outcomes = np.where(outcomes, 1, 0)\n",
    "        plt.subplot(3, 2, 1)\n",
    "        plt.scatter(end_states[:, 0], end_states[:, 1], c=binary_outcomes, cmap=cmap)\n",
    "        plt.title('End States Scatter Plot (Red=Failure, Green=Succes)')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Histogram of Episode Lengths\n",
    "        plt.subplot(3, 2, 2)\n",
    "        plt.hist(analysis_data['episode_lengths'], bins=30, color='skyblue')\n",
    "        plt.title('Histogram of Episode Lengths')\n",
    "        plt.xlabel('Episode Length')\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "        # Line Plot of Episode Rewards\n",
    "        plt.subplot(3, 2, 3)\n",
    "        plt.plot(analysis_data['episode_rewards'], color='green')\n",
    "        plt.title('Episode Rewards Over Time')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "\n",
    "        # Heatmap of State Visits\n",
    "        plt.subplot(3, 2, 4)\n",
    "        state_visits_reshaped = np.reshape(analysis_data['state_visits'], self.num_states)\n",
    "        sns.heatmap(state_visits_reshaped, cmap='viridis', annot=False)\n",
    "        plt.title('Heatmap of State Visits')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "\n",
    "        # Simple plot to display success or failure for each episode\n",
    "        plt.subplot(3, 2, 5)\n",
    "        plt.plot(binary_outcomes, marker='o', linestyle='', color='blue')\n",
    "        plt.title('Success (1) or Failure (0) Per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Outcome (1=Success, 0=Failure)')\n",
    "        plt.yticks([0, 1], ['Failure', 'Success'])\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Line plot for episode lengths\n",
    "        plt.subplot(3, 2, 6)\n",
    "        plt.plot(analysis_data['episode_lengths'], color='purple')\n",
    "        plt.title('Episode Lengths Over Time')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Length of Episode')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # # Adjust layout and show\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f\"analysis_plots_test_{test_id}_{analysis_data['episodes']}_episodes.png\"\n",
    "        plt.savefig(os.path.join(folder, filename))\n",
    "        plt.close()\n",
    "        \n",
    "    \"\"\"Visualize the agent's policy for a specified number of episodes.\"\"\"\n",
    "    def test_policy_with_visualization(self, num_episodes=5):\n",
    "        for episode in range(num_episodes):\n",
    "            state_raw, _ = self.env.reset()\n",
    "            state = self.discretize_state(state_raw)\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                action = np.argmax(self.Q_table[state])\n",
    "                action_taken = \"Turn Left\" if action == 0 else (\"Do Nothing\" if action == 1 else \"Turn Right\")\n",
    "                self.custom_render(step, action_taken)\n",
    "                state_raw, _, done, _, _ = self.env.step(action)\n",
    "                state = self.discretize_state(state_raw)\n",
    "                step += 1\n",
    "           \n",
    "        \"\"\"Custom render function to visualize the state of the environment.\"\"\"\n",
    "    def custom_render(self, step, action_taken):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        positions = np.linspace(self.env.unwrapped.min_position, self.env.unwrapped.max_position, 100)\n",
    "        mountain_heights = np.sin(3 * positions)\n",
    "        plt.plot(positions, mountain_heights, color='gray', linewidth=2)\n",
    "        car_position = self.env.unwrapped.state[0]\n",
    "        plt.scatter(car_position, np.sin(3 * car_position), c='blue', marker='o', s=200)\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.ylabel(\"Mountain Height\")\n",
    "        plt.title(f\"MountainCarEnv - Step {step} - Action {action_taken}\")\n",
    "        plt.xlim(self.env.unwrapped.min_position, self.env.unwrapped.max_position)\n",
    "        plt.ylim(-1.2, 1.2)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8530e5-7e8b-401a-8118-34e35d3c0426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Usage with default values\n",
    "# default_agent = MountainCarAgent()\n",
    "# default_agent.train()\n",
    "# default_agent.plot_and_save_Q_table_analysis()\n",
    "# #default_agent.plot_results()\n",
    "\n",
    "config = {\n",
    "    'alpha': 0.1,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon': 0.3,\n",
    "    'epsilon_decay': 0.9,\n",
    "    'min_epsilon': 0.01,\n",
    "    'num_episodes': 50001,\n",
    "    'num_states': [100,100]\n",
    "}\n",
    "\n",
    "custom_agent = MountainCarAgent(**config)\n",
    "custom_agent.train()\n",
    "custom_agent.plot_and_save_Q_table_analysis(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d86901-7509-46f2-a0f9-1455ea2742eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "succes_rate = custom_agent.test_policy(num_episodes = 100)\n",
    "print(f\"Success rate of the learned policy: {success_rate * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5db884-282b-463c-9b34-f6b075b7d2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysis_data = custom_agent.test_policy_analyzed(num_episodes = 50)\n",
    "custom_agent.plot_analysis_data(analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c882777-b801-486d-8e2f-760f959a6adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_agent.test_policy_with_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "479396b9-1a4f-4202-a815-7ab93ac7ef6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration 1: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [10, 10]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -110099.0\n",
      "Episode: 100, Average Reward: -656.27\n",
      "Episode: 200, Average Reward: -1145.66\n",
      "Episode: 300, Average Reward: -1177.25\n",
      "Episode: 400, Average Reward: -1358.67\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 72.00%\n",
      "Q-table for configuration 1 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 70.00%, Average Episode Length: 508.58 steps, Min Episode Length: 270 steps, Average Reward: -508.58, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 73.60%, Average Episode Length: 484.08 steps, Min Episode Length: 268 steps, Average Reward: -484.08, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 69.70%, Average Episode Length: 513.39 steps, Min Episode Length: 268 steps, Average Reward: -513.39, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 70.22%, Average Episode Length: 508.65 steps, Min Episode Length: 268 steps, Average Reward: -508.65, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 2: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [10, 10]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -36786.0\n",
      "Episode: 100, Average Reward: -766.64\n",
      "Episode: 200, Average Reward: -754.84\n",
      "Episode: 300, Average Reward: -2231.87\n",
      "Episode: 400, Average Reward: -408.26\n",
      "Episode: 500, Average Reward: -1137.34\n",
      "Episode: 600, Average Reward: -2355.37\n",
      "Episode: 700, Average Reward: -529.97\n",
      "Episode: 800, Average Reward: -1182.44\n",
      "Episode: 900, Average Reward: -652.03\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 83.00%\n",
      "Q-table for configuration 2 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 87.00%, Average Episode Length: 275.50 steps, Min Episode Length: 136 steps, Average Reward: -275.50, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 82.60%, Average Episode Length: 306.96 steps, Min Episode Length: 134 steps, Average Reward: -306.96, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 81.80%, Average Episode Length: 314.53 steps, Min Episode Length: 134 steps, Average Reward: -314.53, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 81.88%, Average Episode Length: 316.74 steps, Min Episode Length: 134 steps, Average Reward: -316.74, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 3: {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [10, 10]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -18399.0\n",
      "Episode: 100, Average Reward: -1528.89\n",
      "Episode: 200, Average Reward: -2407.71\n",
      "Episode: 300, Average Reward: -1101.89\n",
      "Episode: 400, Average Reward: -1452.7\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 4.30%\n",
      "Q-table for configuration 3 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 5.00%, Average Episode Length: 982.97 steps, Min Episode Length: 328 steps, Average Reward: -982.97, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 4.20%, Average Episode Length: 982.70 steps, Min Episode Length: 325 steps, Average Reward: -982.70, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 4.50%, Average Episode Length: 981.43 steps, Min Episode Length: 322 steps, Average Reward: -981.43, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 4.30%, Average Episode Length: 982.78 steps, Min Episode Length: 322 steps, Average Reward: -982.78, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 4: {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [10, 10]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -11663.0\n",
      "Episode: 100, Average Reward: -1550.78\n",
      "Episode: 200, Average Reward: -1154.64\n",
      "Episode: 300, Average Reward: -1272.4\n",
      "Episode: 400, Average Reward: -1082.71\n",
      "Episode: 500, Average Reward: -1038.01\n",
      "Episode: 600, Average Reward: -2030.8\n",
      "Episode: 700, Average Reward: -1318.63\n",
      "Episode: 800, Average Reward: -1518.96\n",
      "Episode: 900, Average Reward: -1795.34\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 82.60%\n",
      "Q-table for configuration 4 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 83.00%, Average Episode Length: 292.25 steps, Min Episode Length: 135 steps, Average Reward: -292.25, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 82.40%, Average Episode Length: 301.31 steps, Min Episode Length: 133 steps, Average Reward: -301.31, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 83.80%, Average Episode Length: 294.12 steps, Min Episode Length: 133 steps, Average Reward: -294.12, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 82.95%, Average Episode Length: 302.60 steps, Min Episode Length: 133 steps, Average Reward: -302.60, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 5: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [20, 20]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -19256.0\n",
      "Episode: 100, Average Reward: -1120.31\n",
      "Episode: 200, Average Reward: -434.65\n",
      "Episode: 300, Average Reward: -393.43\n",
      "Episode: 400, Average Reward: -303.47\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 97.40%\n",
      "Q-table for configuration 5 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 96.00%, Average Episode Length: 302.70 steps, Min Episode Length: 174 steps, Average Reward: -302.70, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 97.60%, Average Episode Length: 289.32 steps, Min Episode Length: 177 steps, Average Reward: -289.32, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 97.10%, Average Episode Length: 296.98 steps, Min Episode Length: 146 steps, Average Reward: -296.98, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 97.40%, Average Episode Length: 291.84 steps, Min Episode Length: 146 steps, Average Reward: -291.84, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 6: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [20, 20]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -14602.0\n",
      "Episode: 100, Average Reward: -1262.75\n",
      "Episode: 200, Average Reward: -369.03\n",
      "Episode: 300, Average Reward: -292.81\n",
      "Episode: 400, Average Reward: -351.58\n",
      "Episode: 500, Average Reward: -322.06\n",
      "Episode: 600, Average Reward: -456.59\n",
      "Episode: 700, Average Reward: -285.37\n",
      "Episode: 800, Average Reward: -235.94\n",
      "Episode: 900, Average Reward: -226.87\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 100.00%\n",
      "Q-table for configuration 6 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 163.20 steps, Min Episode Length: 143 steps, Average Reward: -163.20, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 161.67 steps, Min Episode Length: 139 steps, Average Reward: -161.67, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 160.97 steps, Min Episode Length: 139 steps, Average Reward: -160.97, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 161.10 steps, Min Episode Length: 139 steps, Average Reward: -161.10, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 7: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [20, 20]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -18658.0\n",
      "Episode: 500, Average Reward: -517.608\n",
      "Episode: 1000, Average Reward: -217.332\n",
      "Episode: 1500, Average Reward: -194.744\n",
      "Episode: 2000, Average Reward: -163.912\n",
      "Episode: 2500, Average Reward: -184.978\n",
      "Episode: 3000, Average Reward: -184.69\n",
      "Episode: 3500, Average Reward: -153.91\n",
      "Episode: 4000, Average Reward: -171.318\n",
      "Episode: 4500, Average Reward: -143.348\n",
      "Episode: 5000, Average Reward: -141.568\n",
      "Episode: 5500, Average Reward: -135.624\n",
      "Episode: 6000, Average Reward: -140.532\n",
      "Episode: 6500, Average Reward: -143.336\n",
      "Episode: 7000, Average Reward: -165.14\n",
      "Episode: 7500, Average Reward: -152.522\n",
      "Episode: 8000, Average Reward: -144.08\n",
      "Episode: 8500, Average Reward: -140.748\n",
      "Episode: 9000, Average Reward: -138.82\n",
      "Episode: 9500, Average Reward: -145.726\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 98.60%\n",
      "Q-table for configuration 7 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 98.00%, Average Episode Length: 226.53 steps, Min Episode Length: 187 steps, Average Reward: -226.53, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 98.80%, Average Episode Length: 217.29 steps, Min Episode Length: 186 steps, Average Reward: -217.29, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 99.00%, Average Episode Length: 216.11 steps, Min Episode Length: 186 steps, Average Reward: -216.11, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 98.59%, Average Episode Length: 220.39 steps, Min Episode Length: 186 steps, Average Reward: -220.39, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 8: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 25000, 'num_states': [20, 20]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -21296.0\n",
      "Episode: 2500, Average Reward: -256.8376\n",
      "Episode: 5000, Average Reward: -164.9872\n",
      "Episode: 7500, Average Reward: -152.2168\n",
      "Episode: 10000, Average Reward: -148.7288\n",
      "Episode: 12500, Average Reward: -150.9288\n",
      "Episode: 15000, Average Reward: -135.1324\n",
      "Episode: 17500, Average Reward: -154.532\n",
      "Episode: 20000, Average Reward: -139.752\n",
      "Episode: 22500, Average Reward: -147.5408\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 100.00%\n",
      "Q-table for configuration 8 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 140.78 steps, Min Episode Length: 133 steps, Average Reward: -140.78, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 140.86 steps, Min Episode Length: 133 steps, Average Reward: -140.86, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 140.80 steps, Min Episode Length: 133 steps, Average Reward: -140.80, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 140.77 steps, Min Episode Length: 133 steps, Average Reward: -140.77, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 9: {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [20, 20]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -9049.0\n",
      "Episode: 100, Average Reward: -714.13\n",
      "Episode: 200, Average Reward: -338.8\n",
      "Episode: 300, Average Reward: -337.24\n",
      "Episode: 400, Average Reward: -249.1\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 0.00%\n",
      "Q-table for configuration 9 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 0.00%, Average Episode Length: 1000.00 steps, Min Episode Length: 1000 steps, Average Reward: -1000.00, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 0.00%, Average Episode Length: 1000.00 steps, Min Episode Length: 1000 steps, Average Reward: -1000.00, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 0.00%, Average Episode Length: 1000.00 steps, Min Episode Length: 1000 steps, Average Reward: -1000.00, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 0.00%, Average Episode Length: 1000.00 steps, Min Episode Length: 1000 steps, Average Reward: -1000.00, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 10: {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [20, 20]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -16553.0\n",
      "Episode: 100, Average Reward: -849.8\n",
      "Episode: 200, Average Reward: -307.06\n",
      "Episode: 300, Average Reward: -228.56\n",
      "Episode: 400, Average Reward: -189.28\n",
      "Episode: 500, Average Reward: -206.42\n",
      "Episode: 600, Average Reward: -218.02\n",
      "Episode: 700, Average Reward: -225.16\n",
      "Episode: 800, Average Reward: -228.87\n",
      "Episode: 900, Average Reward: -220.71\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 84.50%\n",
      "Q-table for configuration 10 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 83.00%, Average Episode Length: 325.30 steps, Min Episode Length: 158 steps, Average Reward: -325.30, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 82.40%, Average Episode Length: 330.18 steps, Min Episode Length: 158 steps, Average Reward: -330.18, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 83.20%, Average Episode Length: 319.21 steps, Min Episode Length: 158 steps, Average Reward: -319.21, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 82.23%, Average Episode Length: 328.19 steps, Min Episode Length: 158 steps, Average Reward: -328.19, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 11: {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [20, 20]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -15080.0\n",
      "Episode: 500, Average Reward: -335.912\n",
      "Episode: 1000, Average Reward: -198.934\n",
      "Episode: 1500, Average Reward: -181.296\n",
      "Episode: 2000, Average Reward: -210.906\n",
      "Episode: 2500, Average Reward: -162.274\n",
      "Episode: 3000, Average Reward: -194.584\n",
      "Episode: 3500, Average Reward: -177.422\n",
      "Episode: 4000, Average Reward: -168.234\n",
      "Episode: 4500, Average Reward: -185.092\n",
      "Episode: 5000, Average Reward: -169.988\n",
      "Episode: 5500, Average Reward: -167.73\n",
      "Episode: 6000, Average Reward: -183.094\n",
      "Episode: 6500, Average Reward: -196.53\n",
      "Episode: 7000, Average Reward: -193.986\n",
      "Episode: 7500, Average Reward: -169.534\n",
      "Episode: 8000, Average Reward: -192.718\n",
      "Episode: 8500, Average Reward: -186.394\n",
      "Episode: 9000, Average Reward: -192.512\n",
      "Episode: 9500, Average Reward: -168.442\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 96.70%\n",
      "Q-table for configuration 11 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 94.00%, Average Episode Length: 273.47 steps, Min Episode Length: 185 steps, Average Reward: -273.47, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 96.20%, Average Episode Length: 258.52 steps, Min Episode Length: 183 steps, Average Reward: -258.52, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 96.10%, Average Episode Length: 261.04 steps, Min Episode Length: 183 steps, Average Reward: -261.04, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 96.53%, Average Episode Length: 258.15 steps, Min Episode Length: 183 steps, Average Reward: -258.15, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 12: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -54727.0\n",
      "Episode: 100, Average Reward: -1910.23\n",
      "Episode: 200, Average Reward: -943.97\n",
      "Episode: 300, Average Reward: -624.45\n",
      "Episode: 400, Average Reward: -641.75\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 18.40%\n",
      "Q-table for configuration 12 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 30.00%, Average Episode Length: 865.48 steps, Min Episode Length: 259 steps, Average Reward: -865.48, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 20.00%, Average Episode Length: 903.17 steps, Min Episode Length: 259 steps, Average Reward: -903.17, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 20.30%, Average Episode Length: 901.98 steps, Min Episode Length: 259 steps, Average Reward: -901.98, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 18.80%, Average Episode Length: 910.89 steps, Min Episode Length: 256 steps, Average Reward: -910.89, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 13: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -45079.0\n",
      "Episode: 100, Average Reward: -1940.69\n",
      "Episode: 200, Average Reward: -876.78\n",
      "Episode: 300, Average Reward: -760.61\n",
      "Episode: 400, Average Reward: -538.3\n",
      "Episode: 500, Average Reward: -583.2\n",
      "Episode: 600, Average Reward: -527.75\n",
      "Episode: 700, Average Reward: -527.09\n",
      "Episode: 800, Average Reward: -438.7\n",
      "Episode: 900, Average Reward: -442.7\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 99.80%\n",
      "Q-table for configuration 13 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 293.83 steps, Min Episode Length: 185 steps, Average Reward: -293.83, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 99.60%, Average Episode Length: 302.91 steps, Min Episode Length: 184 steps, Average Reward: -302.91, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 99.50%, Average Episode Length: 301.54 steps, Min Episode Length: 170 steps, Average Reward: -301.54, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 99.72%, Average Episode Length: 300.55 steps, Min Episode Length: 170 steps, Average Reward: -300.55, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 14: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -46662.0\n",
      "Episode: 500, Average Reward: -941.982\n",
      "Episode: 1000, Average Reward: -445.398\n",
      "Episode: 1500, Average Reward: -381.738\n",
      "Episode: 2000, Average Reward: -322.542\n",
      "Episode: 2500, Average Reward: -324.266\n",
      "Episode: 3000, Average Reward: -293.858\n",
      "Episode: 3500, Average Reward: -261.698\n",
      "Episode: 4000, Average Reward: -244.836\n",
      "Episode: 4500, Average Reward: -229.676\n",
      "Episode: 5000, Average Reward: -213.494\n",
      "Episode: 5500, Average Reward: -189.826\n",
      "Episode: 6000, Average Reward: -189.418\n",
      "Episode: 6500, Average Reward: -190.822\n",
      "Episode: 7000, Average Reward: -190.01\n",
      "Episode: 7500, Average Reward: -180.728\n",
      "Episode: 8000, Average Reward: -180.726\n",
      "Episode: 8500, Average Reward: -171.08\n",
      "Episode: 9000, Average Reward: -162.618\n",
      "Episode: 9500, Average Reward: -168.97\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 100.00%\n",
      "Q-table for configuration 14 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 139.60 steps, Min Episode Length: 107 steps, Average Reward: -139.60, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 140.46 steps, Min Episode Length: 107 steps, Average Reward: -140.46, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 139.67 steps, Min Episode Length: 107 steps, Average Reward: -139.67, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 139.23 steps, Min Episode Length: 107 steps, Average Reward: -139.23, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 15: {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -43721.0\n",
      "Episode: 2500, Average Reward: -484.0112\n",
      "Episode: 5000, Average Reward: -247.432\n",
      "Episode: 7500, Average Reward: -185.6116\n",
      "Episode: 10000, Average Reward: -170.5304\n",
      "Episode: 12500, Average Reward: -153.616\n",
      "Episode: 15000, Average Reward: -147.3528\n",
      "Episode: 17500, Average Reward: -147.2536\n",
      "Episode: 20000, Average Reward: -154.2596\n",
      "Episode: 22500, Average Reward: -150.4124\n",
      "Episode: 25000, Average Reward: -136.8128\n",
      "Episode: 27500, Average Reward: -141.564\n",
      "Episode: 30000, Average Reward: -143.5324\n",
      "Episode: 32500, Average Reward: -141.6132\n",
      "Episode: 35000, Average Reward: -128.9656\n",
      "Episode: 37500, Average Reward: -126.3948\n",
      "Episode: 40000, Average Reward: -128.7612\n",
      "Episode: 42500, Average Reward: -121.9848\n",
      "Episode: 45000, Average Reward: -121.3496\n",
      "Episode: 47500, Average Reward: -130.4928\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 99.70%\n",
      "Q-table for configuration 15 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 130.61 steps, Min Episode Length: 108 steps, Average Reward: -130.61, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 99.40%, Average Episode Length: 140.18 steps, Min Episode Length: 108 steps, Average Reward: -140.18, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 99.90%, Average Episode Length: 136.75 steps, Min Episode Length: 108 steps, Average Reward: -136.75, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 99.70%, Average Episode Length: 137.41 steps, Min Episode Length: 108 steps, Average Reward: -137.41, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 16: {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -28928.0\n",
      "Episode: 100, Average Reward: -1112.4\n",
      "Episode: 200, Average Reward: -558.66\n",
      "Episode: 300, Average Reward: -475.46\n",
      "Episode: 400, Average Reward: -423.19\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 99.80%\n",
      "Q-table for configuration 16 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 336.77 steps, Min Episode Length: 240 steps, Average Reward: -336.77, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 99.40%, Average Episode Length: 346.19 steps, Min Episode Length: 235 steps, Average Reward: -346.19, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 99.60%, Average Episode Length: 338.25 steps, Min Episode Length: 196 steps, Average Reward: -338.25, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 99.69%, Average Episode Length: 337.89 steps, Min Episode Length: 196 steps, Average Reward: -337.89, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 17: {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -25215.0\n",
      "Episode: 100, Average Reward: -1160.08\n",
      "Episode: 200, Average Reward: -575.11\n",
      "Episode: 300, Average Reward: -473.96\n",
      "Episode: 400, Average Reward: -396.63\n",
      "Episode: 500, Average Reward: -361.89\n",
      "Episode: 600, Average Reward: -402.64\n",
      "Episode: 700, Average Reward: -333.29\n",
      "Episode: 800, Average Reward: -309.09\n",
      "Episode: 900, Average Reward: -298.13\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 100.00%\n",
      "Q-table for configuration 17 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 311.92 steps, Min Episode Length: 238 steps, Average Reward: -311.92, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 317.12 steps, Min Episode Length: 238 steps, Average Reward: -317.12, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 313.92 steps, Min Episode Length: 237 steps, Average Reward: -313.92, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 311.98 steps, Min Episode Length: 237 steps, Average Reward: -311.98, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 18: {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -21225.0\n",
      "Episode: 500, Average Reward: -603.786\n",
      "Episode: 1000, Average Reward: -330.956\n",
      "Episode: 1500, Average Reward: -262.548\n",
      "Episode: 2000, Average Reward: -219.638\n",
      "Episode: 2500, Average Reward: -210.92\n",
      "Episode: 3000, Average Reward: -189.32\n",
      "Episode: 3500, Average Reward: -175.634\n",
      "Episode: 4000, Average Reward: -165.184\n",
      "Episode: 4500, Average Reward: -166.214\n",
      "Episode: 5000, Average Reward: -156.058\n",
      "Episode: 5500, Average Reward: -161.922\n",
      "Episode: 6000, Average Reward: -164.68\n",
      "Episode: 6500, Average Reward: -147.096\n",
      "Episode: 7000, Average Reward: -164.502\n",
      "Episode: 7500, Average Reward: -173.014\n",
      "Episode: 8000, Average Reward: -169.728\n",
      "Episode: 8500, Average Reward: -155.75\n",
      "Episode: 9000, Average Reward: -162.722\n",
      "Episode: 9500, Average Reward: -144.368\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 100.00%\n",
      "Q-table for configuration 18 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 131.13 steps, Min Episode Length: 112 steps, Average Reward: -131.13, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 130.96 steps, Min Episode Length: 111 steps, Average Reward: -130.96, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 131.93 steps, Min Episode Length: 112 steps, Average Reward: -131.93, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 131.41 steps, Min Episode Length: 111 steps, Average Reward: -131.41, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 19: {'alpha': 0.5, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.9, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -15368.0\n",
      "Episode: 500, Average Reward: -526.12\n",
      "Episode: 1000, Average Reward: -289.31\n",
      "Episode: 1500, Average Reward: -223.342\n",
      "Episode: 2000, Average Reward: -203.276\n",
      "Episode: 2500, Average Reward: -190.096\n",
      "Episode: 3000, Average Reward: -191.994\n",
      "Episode: 3500, Average Reward: -171.98\n",
      "Episode: 4000, Average Reward: -176.7\n",
      "Episode: 4500, Average Reward: -183.82\n",
      "Episode: 5000, Average Reward: -183.346\n",
      "Episode: 5500, Average Reward: -174.852\n",
      "Episode: 6000, Average Reward: -172.138\n",
      "Episode: 6500, Average Reward: -141.898\n",
      "Episode: 7000, Average Reward: -144.594\n",
      "Episode: 7500, Average Reward: -162.404\n",
      "Episode: 8000, Average Reward: -173.954\n",
      "Episode: 8500, Average Reward: -158.708\n",
      "Episode: 9000, Average Reward: -147.82\n",
      "Episode: 9500, Average Reward: -142.468\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 100.00%\n",
      "Q-table for configuration 19 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 136.93 steps, Min Episode Length: 114 steps, Average Reward: -136.93, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 137.96 steps, Min Episode Length: 114 steps, Average Reward: -137.96, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 136.55 steps, Min Episode Length: 114 steps, Average Reward: -136.55, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 136.73 steps, Min Episode Length: 114 steps, Average Reward: -136.73, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 20: {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [50, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -28973.0\n",
      "Episode: 2500, Average Reward: -323.3368\n",
      "Episode: 5000, Average Reward: -169.6868\n",
      "Episode: 7500, Average Reward: -161.6872\n",
      "Episode: 10000, Average Reward: -155.6048\n",
      "Episode: 12500, Average Reward: -144.9664\n",
      "Episode: 15000, Average Reward: -131.8468\n",
      "Episode: 17500, Average Reward: -143.5248\n",
      "Episode: 20000, Average Reward: -139.4108\n",
      "Episode: 22500, Average Reward: -137.9748\n",
      "Episode: 25000, Average Reward: -141.5996\n",
      "Episode: 27500, Average Reward: -133.81\n",
      "Episode: 30000, Average Reward: -138.9952\n",
      "Episode: 32500, Average Reward: -129.6456\n",
      "Episode: 35000, Average Reward: -125.5092\n",
      "Episode: 37500, Average Reward: -149.5412\n",
      "Episode: 40000, Average Reward: -128.768\n",
      "Episode: 42500, Average Reward: -140.3288\n",
      "Episode: 45000, Average Reward: -130.9968\n",
      "Episode: 47500, Average Reward: -137.9192\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 100.00%\n",
      "Q-table for configuration 20 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 125.28 steps, Min Episode Length: 111 steps, Average Reward: -125.28, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 123.99 steps, Min Episode Length: 109 steps, Average Reward: -123.99, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 124.01 steps, Min Episode Length: 109 steps, Average Reward: -124.01, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 123.78 steps, Min Episode Length: 109 steps, Average Reward: -123.78, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 21: {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [100, 100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -106860.0\n",
      "Episode: 100, Average Reward: -1969.71\n",
      "Episode: 200, Average Reward: -1139.05\n",
      "Episode: 300, Average Reward: -871.14\n",
      "Episode: 400, Average Reward: -740.41\n",
      "Episode: 500, Average Reward: -721.8\n",
      "Episode: 600, Average Reward: -560.14\n",
      "Episode: 700, Average Reward: -597.92\n",
      "Episode: 800, Average Reward: -523.98\n",
      "Episode: 900, Average Reward: -508.0\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 87.70%\n",
      "Q-table for configuration 21 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 89.00%, Average Episode Length: 577.37 steps, Min Episode Length: 338 steps, Average Reward: -577.37, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 89.40%, Average Episode Length: 577.38 steps, Min Episode Length: 310 steps, Average Reward: -577.38, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 89.20%, Average Episode Length: 585.30 steps, Min Episode Length: 261 steps, Average Reward: -585.30, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 88.02%, Average Episode Length: 589.49 steps, Min Episode Length: 261 steps, Average Reward: -589.49, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Testing configuration 22: {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 25000, 'num_states': [100, 100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -105517.0\n",
      "Episode: 2500, Average Reward: -561.1648\n",
      "Episode: 5000, Average Reward: -296.7532\n",
      "Episode: 7500, Average Reward: -236.7592\n",
      "Episode: 10000, Average Reward: -189.7176\n",
      "Episode: 12500, Average Reward: -165.3032\n",
      "Episode: 15000, Average Reward: -151.94\n",
      "Episode: 17500, Average Reward: -143.6088\n",
      "Episode: 20000, Average Reward: -143.6184\n",
      "Episode: 22500, Average Reward: -142.0572\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 99.80%\n",
      "Q-table for configuration 22 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 141.50 steps, Min Episode Length: 86 steps, Average Reward: -141.50, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 140.76 steps, Min Episode Length: 85 steps, Average Reward: -140.76, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 99.70%, Average Episode Length: 142.55 steps, Min Episode Length: 85 steps, Average Reward: -142.55, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 99.87%, Average Episode Length: 142.31 steps, Min Episode Length: 85 steps, Average Reward: -142.31, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tests for comparation between different approaches\n",
    "\n",
    "configs = [\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [10, 10]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [10, 10]},\n",
    "    \n",
    "    {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [10, 10]},  \n",
    "    {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [10, 10]},  \n",
    "    \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [20, 20]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [20, 20]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [20, 20]},\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 25000, 'num_states': [20, 20]},\n",
    "    \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [20, 20]},  \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [20, 20]},  \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [20, 20]},  \n",
    "    \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [50, 50]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [50, 50]},  \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]},\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [50, 50]},\n",
    "    \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 500, 'num_states': [50, 50]},  \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [50, 50]},  \n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]},\n",
    "    {'alpha': 0.5, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.9, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [50, 50]},\n",
    "    {'alpha': 0.3, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [50, 50]},\n",
    "    \n",
    "    {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 1000, 'num_states': [100, 100]},  \n",
    "    {'alpha': 0.25, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 25000, 'num_states': [100, 100]},\n",
    "    \n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.4, 'epsilon_decay': 0.99, 'min_epsilon': 0.01, 'num_episodes': 100000, 'num_states': [50, 50]},\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.3, 'epsilon_decay': 0.99, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [20, 20]},\n",
    "    {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.1, 'epsilon_decay': 0.95, 'min_epsilon': 0.01, 'num_episodes': 10000, 'num_states': [10, 10]},\n",
    "    {'alpha': 0.25, 'gamma': 0.5, 'epsilon': 0.4, 'epsilon_decay': 0.9, 'min_epsilon': 0.01, 'num_episodes': 50000, 'num_states': [50, 50]},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Test for each configuration\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"Testing configuration {i+1}: {config}\")\n",
    "\n",
    "    # Initialize and train the agent\n",
    "    agent = MountainCarAgent(**config)\n",
    "    agent.train()\n",
    "\n",
    "    # Save Q-table plot\n",
    "    agent.plot_and_save_Q_table_analysis(test_id=i+1)\n",
    "\n",
    "    # Run a simple test with 1000 episodes\n",
    "    success_rate = agent.test_policy(num_episodes=1000)\n",
    "    print(f\"Success rate with 1000 episodes: {success_rate * 100:.2f}%\")\n",
    "    \n",
    "    # Save the Q-table\n",
    "    q_table_filename = f\"q_table_config_{i+1}.npy\"\n",
    "    agent.save_Q_table(q_table_filename)\n",
    "    print(f\"Q-table for configuration {i+1} saved.\")\n",
    "\n",
    "    # Run analysis tests with different number of episodes\n",
    "    for num_episodes in [100, 500, 1000, 10000]:\n",
    "        analysis_data = agent.test_policy_analyzed(num_episodes=num_episodes)\n",
    "        agent.plot_analysis_data(analysis_data, test_id=i+1)\n",
    "        print(f\"Analysis for {num_episodes} episodes saved.\")\n",
    "        print(f\"Success Rate: {analysis_data['success_rate'] * 100:.2f}%, \"\n",
    "              f\"Average Episode Length: {np.mean(analysis_data['episode_lengths']):.2f} steps, \"\n",
    "              f\"Min Episode Length: {np.min(analysis_data['episode_lengths'])} steps, \"\n",
    "              f\"Average Reward: {np.mean(analysis_data['episode_rewards']):.2f}, \"\n",
    "              f\"Failures: {len(analysis_data['failures'])}, \"\n",
    "              f\"Total Episodes: {analysis_data['episodes']}\")\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "20872899-64fa-49a8-84d7-0df119ca54a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Average Reward: -159228.0\n",
      "Episode: 2500, Average Reward: -846.622\n",
      "Episode: 5000, Average Reward: -412.426\n",
      "Episode: 7500, Average Reward: -335.5824\n",
      "Episode: 10000, Average Reward: -298.9316\n",
      "Episode: 12500, Average Reward: -272.9368\n",
      "Episode: 15000, Average Reward: -247.712\n",
      "Episode: 17500, Average Reward: -224.7736\n",
      "Episode: 20000, Average Reward: -202.7004\n",
      "Episode: 22500, Average Reward: -186.3996\n",
      "Episode: 25000, Average Reward: -171.7868\n",
      "Episode: 27500, Average Reward: -162.3468\n",
      "Episode: 30000, Average Reward: -155.0896\n",
      "Episode: 32500, Average Reward: -152.6452\n",
      "Episode: 35000, Average Reward: -150.8672\n",
      "Episode: 37500, Average Reward: -144.6872\n",
      "Episode: 40000, Average Reward: -141.6596\n",
      "Episode: 42500, Average Reward: -144.536\n",
      "Episode: 45000, Average Reward: -141.9188\n",
      "Episode: 47500, Average Reward: -138.1168\n",
      "Episode: 50000, Average Reward: -141.4632\n",
      "Episode: 52500, Average Reward: -142.0348\n",
      "Episode: 55000, Average Reward: -142.5892\n",
      "Episode: 57500, Average Reward: -138.2952\n",
      "Episode: 60000, Average Reward: -132.506\n",
      "Episode: 62500, Average Reward: -132.7924\n",
      "Episode: 65000, Average Reward: -128.6344\n",
      "Episode: 67500, Average Reward: -129.4824\n",
      "Episode: 70000, Average Reward: -138.9948\n",
      "Episode: 72500, Average Reward: -134.2884\n",
      "Episode: 75000, Average Reward: -118.6408\n",
      "Episode: 77500, Average Reward: -124.1364\n",
      "Episode: 80000, Average Reward: -127.61\n",
      "Episode: 82500, Average Reward: -120.0896\n",
      "Episode: 85000, Average Reward: -116.6092\n",
      "Episode: 87500, Average Reward: -116.2572\n",
      "Episode: 90000, Average Reward: -115.0764\n",
      "Episode: 92500, Average Reward: -115.478\n",
      "Episode: 95000, Average Reward: -115.1516\n",
      "Episode: 97500, Average Reward: -118.1748\n",
      "Episode: 100000, Average Reward: -119.208\n",
      "Episode: 102500, Average Reward: -119.9512\n",
      "Episode: 105000, Average Reward: -113.7492\n",
      "Episode: 107500, Average Reward: -111.4572\n",
      "Episode: 110000, Average Reward: -112.6476\n",
      "Episode: 112500, Average Reward: -113.78\n",
      "Episode: 115000, Average Reward: -112.8656\n",
      "Episode: 117500, Average Reward: -112.9296\n",
      "Episode: 120000, Average Reward: -114.4168\n",
      "Episode: 122500, Average Reward: -116.1852\n",
      "Episode: 125000, Average Reward: -113.072\n",
      "Episode: 127500, Average Reward: -109.2044\n",
      "Episode: 130000, Average Reward: -109.816\n",
      "Episode: 132500, Average Reward: -114.768\n",
      "Episode: 135000, Average Reward: -112.9032\n",
      "Episode: 137500, Average Reward: -112.5396\n",
      "Episode: 140000, Average Reward: -114.8284\n",
      "Episode: 142500, Average Reward: -111.6648\n",
      "Episode: 145000, Average Reward: -112.3864\n",
      "Episode: 147500, Average Reward: -113.1424\n",
      "Training complete!\n",
      "Success rate with 1000 episodes: 100.00%\n",
      "Q-table for configuration 24 saved.\n",
      "Analysis for 100 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 124.46 steps, Min Episode Length: 86 steps, Average Reward: -124.46, Failures: 0, Total Episodes: 100\n",
      "Analysis for 500 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 124.48 steps, Min Episode Length: 85 steps, Average Reward: -124.48, Failures: 0, Total Episodes: 500\n",
      "Analysis for 1000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 124.90 steps, Min Episode Length: 85 steps, Average Reward: -124.90, Failures: 0, Total Episodes: 1000\n",
      "Analysis for 10000 episodes saved.\n",
      "Success Rate: 100.00%, Average Episode Length: 125.47 steps, Min Episode Length: 85 steps, Average Reward: -125.47, Failures: 0, Total Episodes: 10000\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Reevaluate a case for later vizualization\n",
    "\n",
    "i = 24\n",
    "config = {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 0.4, 'epsilon_decay': 0.99, 'min_epsilon': 0.01, 'num_episodes': 150000, 'num_states': [100, 100]}\n",
    "\n",
    "\n",
    "# Initialize and train the agent\n",
    "agent = MountainCarAgent(**config)\n",
    "agent.train()\n",
    "\n",
    "# Save Q-table plot\n",
    "agent.plot_and_save_Q_table_analysis(test_id=i)\n",
    "\n",
    "# Run a simple test with 1000 episodes\n",
    "success_rate = agent.test_policy(num_episodes=1000)\n",
    "print(f\"Success rate with 1000 episodes: {success_rate * 100:.2f}%\")\n",
    "\n",
    "# Save the Q-table\n",
    "q_table_filename = f\"q_table_config_{i}.npy\"\n",
    "agent.save_Q_table(q_table_filename)\n",
    "print(f\"Q-table for configuration {i} saved.\")\n",
    "\n",
    "# Run analysis tests with different number of episodes\n",
    "for num_episodes in [100, 500, 1000, 10000]:\n",
    "    analysis_data = agent.test_policy_analyzed(num_episodes=num_episodes)\n",
    "    agent.plot_analysis_data(analysis_data, test_id=i)\n",
    "    print(f\"Analysis for {num_episodes} episodes saved.\")\n",
    "    print(f\"Success Rate: {analysis_data['success_rate'] * 100:.2f}%, \"\n",
    "          f\"Average Episode Length: {np.mean(analysis_data['episode_lengths']):.2f} steps, \"\n",
    "          f\"Min Episode Length: {np.min(analysis_data['episode_lengths'])} steps, \"\n",
    "          f\"Average Reward: {np.mean(analysis_data['episode_rewards']):.2f}, \"\n",
    "          f\"Failures: {len(analysis_data['failures'])}, \"\n",
    "          f\"Total Episodes: {analysis_data['episodes']}\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b055729-f058-4a4f-9452-f6eeeb863082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.test_policy_with_visualization(num_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c957d78-275e-46e3-a6d8-2922fd8f4394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
